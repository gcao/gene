#!/usr/bin/env gene

# LLM Inference Example
# Demonstrates Gene's AI capabilities for language model inference

(println "=== Gene LLM Inference Demo ===")
(println)

# Model configuration
(var model_name "gene_gpt_small")
(var vocab_size 50000)
(var hidden_size 768)
(var num_layers 12)

(println "Model:" model_name)
(println "Vocabulary Size:" vocab_size)
(println "Hidden Size:" hidden_size)
(println "Layers:" num_layers)
(println)

# Initialize AI components
(println "Initializing AI components...")

# Create tokenizer
(var tok (tokenizer/create vocab_size))
(println "  ✓ Tokenizer created")

# Create embeddings
(var embed (embedding/create hidden_size))
(println "  ✓ Embeddings initialized")

# Create model
(var mdl (model/create model_name "gene"))
(println "  ✓ Model loaded")

# Create device
(var dev (device/create "cpu"))
(println "  ✓ Device configured")

(println)

# Tokenization (simulated for demo)
(fn tokenize [text]
  (println "  Tokenizing:" text)
  [101 2023 2003 1037 6291 102])

# Simple forward pass
(fn forward_pass [tokens]
  (println "  Running forward pass...")
  
  # Create input tensor
  (var input_tensor (tensor/create [1 6] :float32))
  
  # Simulate transformer layers
  (for i in (range 0 num_layers)
    (if (< i 3)  # Only print first few layers
      (println "    Processing layer" (+ i 1))))
  
  (println "  Forward pass complete")
  
  # Return output tensor
  (tensor/zeros [1 vocab_size]))

# Text generation
(fn generate [prompt max_tokens]
  (println "Generating text from prompt:" prompt)
  (println)
  
  # Tokenize input
  (var tokens (tokenize prompt))
  
  # Generate tokens
  (print "Output: \"" prompt)
  (for i in (range 0 (min max_tokens 5))
    (var logits (forward_pass tokens))
    (print " ..."))
  (println "\"")
  (println)
  
  tokens)

# Demo
(println "=== Running Inference ===")
(println)

(var result (generate "The future of AI is" 10))
(println "Generated" (- (len result) 6) "new tokens")

(println)
(println "=== Demo Complete ===")
true
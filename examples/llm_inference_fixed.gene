#!/usr/bin/env gene

# Small LLM Inference Example - Fixed version
# Works around the namespace/division parser bug

(println "=== Gene LLM Inference Demo ===")
(println)

# Model configuration
(var model_name "gene_gpt_small")
(var vocab_size 50000)
(var hidden_size 768)
(var num_layers 12)
(var num_heads 12)
(var max_seq_length 512)
(var device_type "cpu")

(println "Model:" model_name)
(println "Parameters: 37 million (approx)")
(println)

# Define wrapper functions to avoid namespace/division issue
(fn create_tokenizer [vocab_size]
  # For now, return a mock tokenizer
  {^type "tokenizer" ^vocab_size vocab_size})

(fn create_embedding [dim]
  # Mock embedding
  {^type "embedding" ^dim dim})

(fn create_model [name type]
  # Mock model
  {^type "model" ^name name ^model_type type})

(fn create_device [type]
  # Mock device
  {^type "device" ^device_type type})

(fn create_tensor [shape]
  # Use actual tensor if available, otherwise mock
  {^type "tensor" ^shape shape})

# 1. Initialize Model Components
(println "Loading model components...")

(var tokenizer (create_tokenizer vocab_size))
(println "  Tokenizer loaded")

(var embeddings (create_embedding hidden_size))
(println "  Embeddings initialized")

(var model (create_model model_name "gene"))
(println "  Model loaded")

(var device_obj (create_device device_type))
(println "  Device:" device_obj)

(println "  Inference session ready")
(println)

# 2. Simple tokenization
(fn tokenize [text]
  (println "Tokenizing:" text)
  [101 2023 2003 1037 6291 102])

# 3. Simple transformer layer
(fn transformer_layer [hidden layer_idx]
  (println "    Layer" layer_idx "processing...")
  hidden)

# 4. Model forward pass
(fn model_forward [input_ids]
  (println "  Running forward pass...")
  
  (var hidden_states [1 (len input_ids) hidden_size])
  
  (loop [i 0 .. num_layers]
    (var hidden_states (transformer_layer hidden_states i)))
  
  (println "  Forward pass complete")
  [1 (len input_ids) vocab_size])

# 5. Text generation
(fn generate_text [prompt max_tokens temperature]
  (println "Generating text...")
  (println "Prompt:" prompt)
  (println "Max tokens:" max_tokens)
  (println "Temperature:" temperature)
  (println)
  
  (var input_ids (tokenize prompt))
  
  (println "Generation:")
  (print "  \"" prompt)
  
  (loop [step 0 .. max_tokens]
    (var logits (model_forward input_ids))
    (print ".")
    (var input_ids (append input_ids 1234))
    
    (if (= step 5)
      (break)))
  
  (println "\"")
  (println)
  
  [1234 5678])

# 6. Main Demo
(println "=== DEMO SCENARIOS ===")
(println)

(println "1. Text Completion")
(println "-------------------")
(generate_text "The future of AI is" 10 0.8)

(println "=== Summary ===")
(println "Gene LLM Inference Demonstration Complete!")
(println)

true